{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbender Tutorial 1: Iris Flower Classification\n",
    "\n",
    "Now that we have a good understanding of airbender's structure, let's consider an example with the Iris flower dataset. The dataset includes four features, listed below, and three classes we will try to predict. The goal is to build a model that can effectively determine the breed of the flower using only the length and width metrics provided in the feature set. \n",
    "\n",
    "```\n",
    "Iris plants dataset\n",
    "--------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information (Features):\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "    :class (type of flower):\n",
    "        - Setosa\n",
    "        - Versicolour\n",
    "        - Virginica\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Before we start, let's import Pandas and Airbender's `DagLayer` class so we can validate our steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sshowalter/repos/airbender/tutorials/iris\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "#Import DagLayer from airbender\n",
    "from airbender.dag.layers import DagLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "---------------------------------------------\n",
    "\n",
    "Since this dataset is small and fairly simple, all we need to do is import the dataset. Airbender has provided a link to the dataset in the `tutorial` folder. To incorporate this into the experiment, we only need to write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = {'iris':         #Tag\n",
    "                    DagLayer(\n",
    "            {'https://raw.githubusercontent.com/SamShowalter/WMP_training/master/airbender_iris_demo.csv': \\\n",
    "             {pd.read_csv: {'sep': ','}},\n",
    "\n",
    "                                }\n",
    "                            )\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA for missing data and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "---------------------------------------------\n",
    "\n",
    "One of the largest benefits of Airbender is the control is offers to ensure unbiased experimentation. Right after the data is read into Airbender, it is split into `train` and `test` datasets. For every `preprocessing` and `feature_engineering` operation, the train dataset is operated on first. If the operation had any artifacts (e.g. median imputation takes the median of the training dataset as its input value), those values **are passed to the testing dataset and applied directly**. In this way, the experiment is much more likely to be free of information leak.\n",
    "\n",
    "Right now, Airbender only accommodates traditional train test splits with a single test slice. K-fold cross validation is coming as a new feature shortly, as is the ability to select a validation slice of data as well.\n",
    "\n",
    "For our Iris example, we will take a random, 25% slice of the data for our testing set. The configuration is outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import splitting functionality\n",
    "from airbender.static.splitting import train_test_split\n",
    "\n",
    "splitting = {'split':\n",
    "\n",
    "                            DagLayer({'sklearn': {train_test_split: {\"target\": \"flower_label\",\n",
    "                                                                    \"test_ratio\": 0.25,\n",
    "                                                                    \"random_state\": 42}\n",
    "                                                 }\n",
    "                                    })\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "---------------------------------------------\n",
    "\n",
    "In this edited dataset, we can see that there are some missing values. The original Iris dataset did not have missing values, but we have added them artificially to better simulate an actual dataset. \n",
    "\n",
    "To accommodate these missing values, we will use median imputation, provided by airbender. We need to use Airbender's label encoder function as label_encoder's ensure we attribute the same numeric label for each class across the train and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from airbender.static.preprocessing import impute\n",
    "\n",
    "preprocessing = {'missing_data':\n",
    "\n",
    "                            DagLayer({\n",
    "                                        # tag name             # Operator Family\n",
    "                                        'median_impute':       {impute: {'method': 'median'}}\n",
    "                                    })\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---------------------------------------------\n",
    "\n",
    "Feature engineering is the DagLayer where you can apply functions or series of function to specific data columns. You can also pass through columns that are already in a suitable modeling format by simply putting `None` in place of the operator and parameters dictionary.\n",
    "\n",
    "Based on our EDA, we noticed that `petal_length` appears to have one or more outliers. We will handle those by winsorizing them with 5%-95% bounds, then normalize the data. To demonstrate how to simply pass columns through to modeling, we will assume `sepal_length` and `petal_width` do not require feature engineering. \n",
    "\n",
    "Lastly, we will need to encode `flower_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering Imports\n",
    "from airbender.static.feature_engineering import normalize_values, winsorize, encode_labels\n",
    "\n",
    "feature_engineering = {'col_transformations':\n",
    "\n",
    "                            DagLayer({\n",
    "                                        # Column name             # Operator Family\n",
    "\n",
    "                                        'sepal_width':            {normalize_values: None},\n",
    "                                        'sepal_length':           None,\n",
    "                                        'petal_length':           {winsorize:            {'limits': [0.05, 0.05]},\n",
    "                                                                   normalize_values:     None},\n",
    "                                        #Pass-through\n",
    "                                        'petal_width':            None,\n",
    "                                        'flower_label':           {encode_labels: None}\n",
    "                                    })\n",
    "                      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "---------------------------------------------\n",
    "\n",
    "The modeling sublayer currently follows a `fit` and `predict` interface, meaning all `scikit-learn` models and many additional algorithms like `lightgbm` and `xgboost` are compatible. Deep learning models are not yet compatible. Airbender currently only supports supervised algorithms as well. You do not need to instantiate the models you provide. Airbender will do this for you.\n",
    "\n",
    "For the Iris dataset, we will examine three models, all from `scikit-learn`: Random Forest, Support Vector Machine, and Logistic Regression. The configuration for this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import models from Sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modeling = {'modeling': \n",
    "                \n",
    "                DagLayer({\n",
    "                            'LOG': {LogisticRegression:         {'solver':'lbfgs'}},           \n",
    "                            'RF':  {RandomForestClassifier:     {'n_estimators': 10}},\n",
    "                            'SVM': {SVC:                        {'kernel': 'linear'}}\n",
    "                        })  \n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---------------------------------------------\n",
    "\n",
    "Lastly, we need a way to determine which model is best suited to predict on this dataset. Therefore, we will provide our Airbender evaluation DagLayer with performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "evaluation = {'metrics':\n",
    "                    \n",
    "                     DagLayer({\n",
    "                                'acc':            {accuracy_score: None},\n",
    "                                'recall':         {precision_score: {'average': 'weighted'}},\n",
    "                                'precision':      {recall_score: {'average': 'weighted'}},\n",
    "                                'f1':             {f1_score: {'average': 'weighted'}}\n",
    "                             })\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate all Configurations\n",
    "---------------------------------------------\n",
    "\n",
    "Now that we have all of the functionality we need to run an experiment with the Iris dataset, we need to consolidate that into a single configuration object. This is typically done with the following structure. Note, you can write these steps in any order, as the conceptual DAG configuration will ensure correct order (e.g. `data_sources` is first, `evaluation` last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_config = {\n",
    "                'data_sources':            data_sources,\n",
    "                'splitting':               splitting,\n",
    "                'preprocessing':           preprocessing,\n",
    "                'feature_engineering':     feature_engineering,\n",
    "                'modeling':                modeling,\n",
    "                'evaluation':              evaluation\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done! We need to add a few final arguments to label our experiment and send the correct metadata to developers and users. First, we need to give the experiment a `dag_name`, shown below. The `dag` argument is a list of the parameters the user wants to pass directly to airflow about the management of its execution. More information about this section can be found in the Airflow documentation [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbender_config = { \n",
    "                        'dag_name': \"Airbender_Iris_Tutorial\",\n",
    "                        \n",
    "                        'dag':      {\n",
    "                                        'owner': 'airbender',\n",
    "                                        # 'email': [<EMAIL>, <EMAIL>, ...],\n",
    "                                        # 'op_args':{},\n",
    "                                        # 'op_kwargs': {},\n",
    "                                    },\n",
    "                                    \n",
    "                        #DAG configuration we just created\n",
    "                        'config' : iris_config\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All finished! Now we are ready to generate the code for our Airbender DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Airbender DAG\n",
    "---------------------------------------------\n",
    "\n",
    "Once we have written a valid Airbender configuration, generating the code for the DAG is incredibly simple. All you need to do is give the configuration to Airbender's DAG generator, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying Ordered Dag Layers with Tags:\n",
      "\n",
      "0 ['data_sources', 'iris']\n",
      "1 ['splitting', 'split']\n",
      "2 ['preprocessing', 'missing_data']\n",
      "3 ['feature_engineering', 'col_transformations']\n",
      "4 ['modeling', 'modeling']\n",
      "5 ['evaluation', 'metrics']\n",
      "\n",
      "Generated airflow file with name: Airbender_Iris_Tutorial_airbender_10-28-2019--14.11.49.py\n"
     ]
    }
   ],
   "source": [
    "#DAG Generator Imports\n",
    "from airbender.dag.generator import DagGenerator\n",
    "\n",
    "#Generate File\n",
    "dg = DagGenerator(airbender_config)\n",
    "dg.generate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our experiment has been converted into a DAG that is ready to run on Airflow. Note that Airbender intelligently imported all of the functions and classes we used in our configuration into the final file. You can view the file we just generated [here](<LINK TO OUTPUT FILE>). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and Run DAG in Airflow\n",
    "--------------------------------------------\n",
    "\n",
    "At this point, our job is finished. All we need to do is place this file into our airflow dag directory, turn on Airflow's scheduler and webserver, and watch the experiment run. While the experiment is running, you can track the progress of the DAG in real-time by with Airflow's tree or graph view, shown below.\n",
    "\n",
    "<IMAGE OF AIRFLOW DAG>\n",
    "    \n",
    "You can also analyze the content, input, output, and metadata of specific tasks by clicking on them and viewing the Airflow log. Airbender wraps all of your functions in decorators to ensure compatibility, but the task id will indicate which functions were run. If more information is needed, you may find it in the `params` log after clicking on a specific task instance, shown below.\n",
    "\n",
    "IMAGE OF TASK\n",
    "\n",
    "IMAGE OF TASK PARAMETERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
