{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def is_fitted(model):\n",
    "    \"\"\"Checks if model object has any attributes ending with an underscore\"\"\"\n",
    "    return 0 < len( [k for k,v in inspect.getmembers(model) if k.endswith('_') and not k.startswith('__')] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.preprocessing.data import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import json\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from collections import Counter\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.linear_model.logistic import LogisticRegression\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_dynamically(obj):\n",
    "    import_statement = None\n",
    "    try:\n",
    "        import_statement = \"from {} import {}\\n\".format(\n",
    "                                                      obj.__module__,\n",
    "                                                      obj.__name__)\n",
    "    except Exception as e:\n",
    "            print(\"Dynamic import failed. Trying simpler import.\")\n",
    "            import_statement = 'import {}\\n'.format(obj.__name__)\n",
    "    \n",
    "    return import_statement\n",
    "    \n",
    "import_dynamically(GaussianNB)\n",
    "import_dynamically(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /mnt/c/Users/<ubuntu.username>/Pictures\n",
    "import pandas as pd\n",
    "pd.get_dummies.__module__\n",
    "import pandas\n",
    "from scipy.stats import *\n",
    "from pandas.core.reshape.reshape import get_dummies\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_dynamically(find_missing_data)\n",
    "# import_dynamically(normalize_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../repos/WMP_ML_MVP/\")\n",
    "\n",
    "from wmp_ml.static.missing_data import *\n",
    "from wmp_ml.static.feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dictionary = something belongs to something else, is kwargs, or has certain order\n",
    "list = collection of several parallel operations\n",
    "\n",
    "blocks -> first level keys (THESE HAVE SET ORDER)\n",
    "Below this ^^^ -> The order you provide the code will matter\n",
    "    - dict = matters\n",
    "    - list = does not matter\n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import *\n",
    "from scipy.stats import *\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kwargs = \\\n",
    "{   'meta': {\n",
    "             'doc': '''# Sam Showalter's Generated DAG\n",
    "If this ever compiles in airflow, POP THA CHAMPAGNE!\n",
    "\n",
    "## Objective: Classification\n",
    "'''},\n",
    "     'modeling': { \n",
    "                    'models': {LogisticRegression: None, GaussianNB: None},              \n",
    "                 },\n",
    "     'data_sources': {\n",
    "                     pd.read_csv:{'https://raw.githubusercontent.com/SamShowalter/WMP_training/01_EDA/data/IBM_Employee_Attrition.csv': None,\n",
    "                                  } \n",
    "                     },\n",
    " \n",
    "     \n",
    "     #Expects flat dataset\n",
    "     'splitting': {\n",
    "                        train_test_split  : {#'target': \"Attrition\",\n",
    "                                                  'test_size': 0.2,\n",
    "                                                  'random_state': 42},\n",
    "                  },\n",
    " \n",
    "     \n",
    "     #Expects flat dataset\n",
    "     'preprocessing': { \n",
    "                        'missing_data': {\n",
    "                                              'default': \"impute_median\"                                          \n",
    "                                        },\n",
    "                        'outliers': {   \n",
    "                                              'detect': {'threshold': 6},\n",
    "                                              'default': \"winsorize\"\n",
    "                                    }\n",
    "                        \n",
    "                     },\n",
    " \n",
    "    #Expects flat dataset\n",
    "    'feature_engineering': {\n",
    "                            'transformations': {\n",
    "                                                    ('Department', 'EducationField', 'JobRole', \"MaritalStatus\"): {pd.get_dummies: None},\n",
    "                                                    'BusinessTravel': {create_ordinal_df: {'ordinal_names':{\"Non-Travel\" :0, \"Travel_Rarely\":1, \"Travel_Frequently\": 2}}},\n",
    "                                                    ('Attrition', 'Gender', 'OverTime') : {convert_boolean_df: {'boolean_names_and_values':[[\"Attrition\", \"Yes\", \"No\"],\n",
    "                                                                                                                  [\"Gender\", \"Male\", \"Female\"],\n",
    "                                                                                                                   [\"OverTime\", \"Yes\", \"No\"]], \n",
    "                                                                                                                'tag' : None}},\n",
    "                                                    ('Age', 'DistanceFromHome', 'EmployeeCount', \"EnvironmentSatisfaction\",\n",
    "                                                    'JobInvolvement', 'JobSatisfaction', \"MonthlyIncome\",\n",
    "                                                    'NumCompaniesWorked', 'PercentSalaryHike', \"PerformanceRating\",\n",
    "                                                    'RelationshipSatisfaction', 'StandardHours', 'TotalWorkingYears',\n",
    "                                                    'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
    "                                                    'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "                                                    'YearsWithCurrManager') : {normalize_values: {'transformer': StandardScaler, \n",
    "                                                                                                  'tag': None}},\n",
    "                                                    'MonthlyRate': {yeojohnson: None,\n",
    "                                                                    normalize_values: {'transformer': StandardScaler,\n",
    "                                                                                       'tag': None}},\n",
    "                                               },\n",
    "                            \n",
    "                           },\n",
    " \n",
    "     #Expects predictions\n",
    "     'evaluation':{'metrics' : {accuracy_score: None, f1_score: None, precision_score: None, confusion_matrix: None, \n",
    "                   classification_report: None, recall_score :None},\n",
    "                   'visualization' : None}\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.linear_model.logistic import LogisticRegression\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from pandas.io.parsers import read_csv\n",
      "from sklearn.model_selection._split import train_test_split\n",
      "from pandas.core.reshape.reshape import get_dummies\n",
      "from wmp_ml.static.feature_engineering import create_ordinal_df\n",
      "from wmp_ml.static.feature_engineering import convert_boolean_df\n",
      "from wmp_ml.static.feature_engineering import normalize_values\n",
      "from sklearn.preprocessing.data import StandardScaler\n",
      "from scipy.stats.morestats import yeojohnson\n",
      "from sklearn.metrics.classification import accuracy_score\n",
      "from sklearn.metrics.classification import f1_score\n",
      "from sklearn.metrics.classification import precision_score\n",
      "from sklearn.metrics.classification import confusion_matrix\n",
      "from sklearn.metrics.classification import classification_report\n",
      "from sklearn.metrics.classification import recall_score\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rec_imports(config_section, import_list, import_check):\n",
    "    \n",
    "    try:\n",
    "        for key in config_section.keys():\n",
    "            #print(key)\n",
    "            if any([inspect.isclass(key),\n",
    "                   inspect.isfunction(key),\n",
    "                   inspect.ismodule(key)]) and key not in import_check:\n",
    "                import_list.append(import_dynamically(key))\n",
    "                import_check.add(key)\n",
    "            value = config_section[key]\n",
    "            \n",
    "            if any([inspect.isclass(value),\n",
    "                   inspect.isfunction(value),\n",
    "                   inspect.ismodule(value)]) and value not in import_check:\n",
    "                import_list.append(import_dynamically(value))\n",
    "                import_check.add(value)\n",
    "            \n",
    "            rec_imports(config_section[key], import_list, import_check)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "            \n",
    "\n",
    "def write_imports(config):\n",
    "    import_list = []\n",
    "    import_check = set()\n",
    "    for key in config:\n",
    "        if isinstance(config[key], dict):\n",
    "            rec_imports(config[key], import_list, import_check)\n",
    "    \n",
    "    return \"\".join([item for item in import_list if item != \"\"])\n",
    "\n",
    "print(write_imports(kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': ['something', 'new_something'], 'astest': ['new_something']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "d['test'] = ['something']\n",
    "d.setdefault('test', []).append('new_something')\n",
    "d.setdefault('astest', []).append('new_something')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwarg_dict = collections.OrderedDict(kwargs)\n",
    "# # for item in kwarg_dict.keys():\n",
    "# #     print(kwarg_dict[item])\n",
    "\n",
    "# #test = collections.OrderedDict(kwarg_dict['preprocessing'])\n",
    "# for key in kwarg_dict['preprocessing']:\n",
    "#     if isinstance(key, tuple):\n",
    "#         for item in key:\n",
    "#             print(item)\n",
    "#     else:\n",
    "#         print(kwarg_dict['preprocessing'][key])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/SamShowalter/WMP_training/01_EDA/data/IBM_Employee_Attrition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279.0</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591.0</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely     1102.0                   Sales   \n",
       "1   49        No  Travel_Frequently      279.0  Research & Development   \n",
       "2   37       Yes      Travel_Rarely     1373.0  Research & Development   \n",
       "3   33        No  Travel_Frequently     1392.0  Research & Development   \n",
       "4   27        No      Travel_Rarely      591.0  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0               1.0          2  Life Sciences              1             1.0   \n",
       "1               8.0          1  Life Sciences              1             2.0   \n",
       "2               2.0          2          Other              1             4.0   \n",
       "3               3.0          4  Life Sciences              1             5.0   \n",
       "4               2.0          1        Medical              1             7.0   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provided function for detecting outliers\n",
    "def detect_outliers(col_names, data, std_thresh = 6):\n",
    "    \n",
    "    for column in col_names:\n",
    "        \n",
    "        #Create z_score proxy for each column\n",
    "        data['z_score'] = np.absolute(zscore(data[column]))\n",
    "        \n",
    "        #Determine if there are outliers, as defined by z_score threshold\n",
    "        outliers = data.loc[data.z_score > std_thresh, [column, 'z_score']]\n",
    "        \n",
    "        #If there are no outliers\n",
    "        if outliers.shape[0] == 0:\n",
    "            print(\"No outliers for column {} at threshold of {} stdevs\".format(column, std_thresh))\n",
    "        \n",
    "        #If there are outliers\n",
    "        else:\n",
    "            print(\"\\n {} outlier(s) found for column {} at threshold of {} stdevs. See below\".format(outliers.shape[0],\n",
    "                                                                                        column, std_thresh))\n",
    "            print(\"\\n\")\n",
    "            print(outliers)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        #Drop z_score from data\n",
    "        data.drop('z_score', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'sklearn.linear_model.logistic',\n",
       "              '__doc__': 'Logistic Regression (aka logit, MaxEnt) classifier.\\n\\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\\n    scheme if the \\'multi_class\\' option is set to \\'ovr\\', and uses the\\n    cross-entropy loss if the \\'multi_class\\' option is set to \\'multinomial\\'.\\n    (Currently the \\'multinomial\\' option is supported only by the \\'lbfgs\\',\\n    \\'sag\\', \\'saga\\' and \\'newton-cg\\' solvers.)\\n\\n    This class implements regularized logistic regression using the\\n    \\'liblinear\\' library, \\'newton-cg\\', \\'sag\\', \\'saga\\' and \\'lbfgs\\' solvers. **Note\\n    that regularization is applied by default**. It can handle both dense\\n    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\\n    floats for optimal performance; any other input format will be converted\\n    (and copied).\\n\\n    The \\'newton-cg\\', \\'sag\\', and \\'lbfgs\\' solvers support only L2 regularization\\n    with primal formulation, or no regularization. The \\'liblinear\\' solver\\n    supports both L1 and L2 regularization, with a dual formulation only for\\n    the L2 penalty. The Elastic-Net regularization is only supported by the\\n    \\'saga\\' solver.\\n\\n    Read more in the :ref:`User Guide <logistic_regression>`.\\n\\n    Parameters\\n    ----------\\n    penalty : str, \\'l1\\', \\'l2\\', \\'elasticnet\\' or \\'none\\', optional (default=\\'l2\\')\\n        Used to specify the norm used in the penalization. The \\'newton-cg\\',\\n        \\'sag\\' and \\'lbfgs\\' solvers support only l2 penalties. \\'elasticnet\\' is\\n        only supported by the \\'saga\\' solver. If \\'none\\' (not supported by the\\n        liblinear solver), no regularization is applied.\\n\\n        .. versionadded:: 0.19\\n           l1 penalty with SAGA solver (allowing \\'multinomial\\' + L1)\\n\\n    dual : bool, optional (default=False)\\n        Dual or primal formulation. Dual formulation is only implemented for\\n        l2 penalty with liblinear solver. Prefer dual=False when\\n        n_samples > n_features.\\n\\n    tol : float, optional (default=1e-4)\\n        Tolerance for stopping criteria.\\n\\n    C : float, optional (default=1.0)\\n        Inverse of regularization strength; must be a positive float.\\n        Like in support vector machines, smaller values specify stronger\\n        regularization.\\n\\n    fit_intercept : bool, optional (default=True)\\n        Specifies if a constant (a.k.a. bias or intercept) should be\\n        added to the decision function.\\n\\n    intercept_scaling : float, optional (default=1)\\n        Useful only when the solver \\'liblinear\\' is used\\n        and self.fit_intercept is set to True. In this case, x becomes\\n        [x, self.intercept_scaling],\\n        i.e. a \"synthetic\" feature with constant value equal to\\n        intercept_scaling is appended to the instance vector.\\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\\n\\n        Note! the synthetic feature weight is subject to l1/l2 regularization\\n        as all other features.\\n        To lessen the effect of regularization on synthetic feature weight\\n        (and therefore on the intercept) intercept_scaling has to be increased.\\n\\n    class_weight : dict or \\'balanced\\', optional (default=None)\\n        Weights associated with classes in the form ``{class_label: weight}``.\\n        If not given, all classes are supposed to have weight one.\\n\\n        The \"balanced\" mode uses the values of y to automatically adjust\\n        weights inversely proportional to class frequencies in the input data\\n        as ``n_samples / (n_classes * np.bincount(y))``.\\n\\n        Note that these weights will be multiplied with sample_weight (passed\\n        through the fit method) if sample_weight is specified.\\n\\n        .. versionadded:: 0.17\\n           *class_weight=\\'balanced\\'*\\n\\n    random_state : int, RandomState instance or None, optional (default=None)\\n        The seed of the pseudo random number generator to use when shuffling\\n        the data.  If int, random_state is the seed used by the random number\\n        generator; If RandomState instance, random_state is the random number\\n        generator; If None, the random number generator is the RandomState\\n        instance used by `np.random`. Used when ``solver`` == \\'sag\\' or\\n        \\'liblinear\\'.\\n\\n    solver : str, {\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\', \\'sag\\', \\'saga\\'},              optional (default=\\'liblinear\\').\\n\\n        Algorithm to use in the optimization problem.\\n\\n        - For small datasets, \\'liblinear\\' is a good choice, whereas \\'sag\\' and\\n          \\'saga\\' are faster for large ones.\\n        - For multiclass problems, only \\'newton-cg\\', \\'sag\\', \\'saga\\' and \\'lbfgs\\'\\n          handle multinomial loss; \\'liblinear\\' is limited to one-versus-rest\\n          schemes.\\n        - \\'newton-cg\\', \\'lbfgs\\', \\'sag\\' and \\'saga\\' handle L2 or no penalty\\n        - \\'liblinear\\' and \\'saga\\' also handle L1 penalty\\n        - \\'saga\\' also supports \\'elasticnet\\' penalty\\n        - \\'liblinear\\' does not handle no penalty\\n\\n        Note that \\'sag\\' and \\'saga\\' fast convergence is only guaranteed on\\n        features with approximately the same scale. You can\\n        preprocess the data with a scaler from sklearn.preprocessing.\\n\\n        .. versionadded:: 0.17\\n           Stochastic Average Gradient descent solver.\\n        .. versionadded:: 0.19\\n           SAGA solver.\\n        .. versionchanged:: 0.20\\n            Default will change from \\'liblinear\\' to \\'lbfgs\\' in 0.22.\\n\\n    max_iter : int, optional (default=100)\\n        Maximum number of iterations taken for the solvers to converge.\\n\\n    multi_class : str, {\\'ovr\\', \\'multinomial\\', \\'auto\\'}, optional (default=\\'ovr\\')\\n        If the option chosen is \\'ovr\\', then a binary problem is fit for each\\n        label. For \\'multinomial\\' the loss minimised is the multinomial loss fit\\n        across the entire probability distribution, *even when the data is\\n        binary*. \\'multinomial\\' is unavailable when solver=\\'liblinear\\'.\\n        \\'auto\\' selects \\'ovr\\' if the data is binary, or if solver=\\'liblinear\\',\\n        and otherwise selects \\'multinomial\\'.\\n\\n        .. versionadded:: 0.18\\n           Stochastic Average Gradient descent solver for \\'multinomial\\' case.\\n        .. versionchanged:: 0.20\\n            Default will change from \\'ovr\\' to \\'auto\\' in 0.22.\\n\\n    verbose : int, optional (default=0)\\n        For the liblinear and lbfgs solvers set verbose to any positive\\n        number for verbosity.\\n\\n    warm_start : bool, optional (default=False)\\n        When set to True, reuse the solution of the previous call to fit as\\n        initialization, otherwise, just erase the previous solution.\\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\\n\\n        .. versionadded:: 0.17\\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\\n\\n    n_jobs : int or None, optional (default=None)\\n        Number of CPU cores used when parallelizing over classes if\\n        multi_class=\\'ovr\\'\". This parameter is ignored when the ``solver`` is\\n        set to \\'liblinear\\' regardless of whether \\'multi_class\\' is specified or\\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\\n        context. ``-1`` means using all processors.\\n        See :term:`Glossary <n_jobs>` for more details.\\n\\n    l1_ratio : float or None, optional (default=None)\\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\\n        used if ``penalty=\\'elasticnet\\'`. Setting ``l1_ratio=0`` is equivalent\\n        to using ``penalty=\\'l2\\'``, while setting ``l1_ratio=1`` is equivalent\\n        to using ``penalty=\\'l1\\'``. For ``0 < l1_ratio <1``, the penalty is a\\n        combination of L1 and L2.\\n\\n    Attributes\\n    ----------\\n\\n    classes_ : array, shape (n_classes, )\\n        A list of class labels known to the classifier.\\n\\n    coef_ : array, shape (1, n_features) or (n_classes, n_features)\\n        Coefficient of the features in the decision function.\\n\\n        `coef_` is of shape (1, n_features) when the given problem is binary.\\n        In particular, when `multi_class=\\'multinomial\\'`, `coef_` corresponds\\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\\n\\n    intercept_ : array, shape (1,) or (n_classes,)\\n        Intercept (a.k.a. bias) added to the decision function.\\n\\n        If `fit_intercept` is set to False, the intercept is set to zero.\\n        `intercept_` is of shape (1,) when the given problem is binary.\\n        In particular, when `multi_class=\\'multinomial\\'`, `intercept_`\\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\\n        outcome 0 (False).\\n\\n    n_iter_ : array, shape (n_classes,) or (1, )\\n        Actual number of iterations for all classes. If binary or multinomial,\\n        it returns only 1 element. For liblinear solver, only the maximum\\n        number of iteration across all classes is given.\\n\\n        .. versionchanged:: 0.20\\n\\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.datasets import load_iris\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> X, y = load_iris(return_X_y=True)\\n    >>> clf = LogisticRegression(random_state=0, solver=\\'lbfgs\\',\\n    ...                          multi_class=\\'multinomial\\').fit(X, y)\\n    >>> clf.predict(X[:2, :])\\n    array([0, 0])\\n    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS\\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\\n           [9.7...e-01, 2.8...e-02, ...e-08]])\\n    >>> clf.score(X, y)\\n    0.97...\\n\\n    See also\\n    --------\\n    SGDClassifier : incrementally trained logistic regression (when given\\n        the parameter ``loss=\"log\"``).\\n    LogisticRegressionCV : Logistic regression with built-in cross validation\\n\\n    Notes\\n    -----\\n    The underlying C implementation uses a random number generator to\\n    select features when fitting the model. It is thus not uncommon,\\n    to have slightly different results for the same input data. If\\n    that happens, try with a smaller tol parameter.\\n\\n    Predict output may not match that of standalone liblinear in certain\\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\\n    in the narrative documentation.\\n\\n    References\\n    ----------\\n\\n    LIBLINEAR -- A Library for Large Linear Classification\\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\\n\\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\\n        Minimizing Finite Sums with the Stochastic Average Gradient\\n        https://hal.inria.fr/hal-00860051/document\\n\\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\\n        SAGA: A Fast Incremental Gradient Method With Support\\n        for Non-Strongly Convex Composite Objectives\\n        https://arxiv.org/abs/1407.0202\\n\\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\\n        methods for logistic regression and maximum entropy models.\\n        Machine Learning 85(1-2):41-75.\\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\\n    ',\n",
       "              '__init__': <function sklearn.linear_model.logistic.LogisticRegression.__init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)>,\n",
       "              'fit': <function sklearn.linear_model.logistic.LogisticRegression.fit(self, X, y, sample_weight=None)>,\n",
       "              'predict_proba': <function sklearn.linear_model.logistic.LogisticRegression.predict_proba(self, X)>,\n",
       "              'predict_log_proba': <function sklearn.linear_model.logistic.LogisticRegression.predict_log_proba(self, X)>})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../repos/WMP_ML_MVP/wmp_ml/static\")\n",
    "sys.path.append(\"../repos/WMP_ML_MVP/wmp_ml/orch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dag_generator import DagGenerator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here info not {here}\n"
     ]
    }
   ],
   "source": [
    "print(\"Here {} not {{here}}\".format(\"info\", {\"here\": \"It worked\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_args = { \n",
    "                'dag_name': \"WMP_ML_test_creation\",\n",
    "                'dag':      {\n",
    "                                'owner': 'Sam Showalter',\n",
    "                                'email': ['sshowalter@wmp.com'],\n",
    "                                'op_args':{},\n",
    "                                'op_kwargs': {},\n",
    "                                'params': {}\n",
    "                            },\n",
    "                'config' : kwargs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DagGenerator(dag_args)\n",
    "dg.generate_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd ={\n",
    "    \"data_sources\": 0,\n",
    "    \"eda\": 1,\n",
    "    \"splitting\": 2,\n",
    "    \"preprocessing\": 3,\n",
    "    \"feature_engineering\": 4,\n",
    "    \"modeling\": 5,\n",
    "    \"evaluation\": 6,\n",
    "    \"visualization\": 7,\n",
    "    \"storage\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def set_default(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    raise TypeError\n",
    "\n",
    "with open('../repos/WMP_ML_MVP/wmp_ml/config/dag_hierarchy.cfg', 'w') as config:\n",
    "    config.write(json.dumps(dd, indent=4))\n",
    "    config.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sshowalter/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'adsf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adsfmareae'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += \"mareae\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-c7ee7738d0c2>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-c7ee7738d0c2>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    {'dimensionality_reduction': {1: ['pca', 'svd']}}},\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "config = {   'splitting': {2: ['train_test_split', 'k_fold']},\n",
    "             'data_souces': {0: {'csv': 0, 'sql': 1, 'pickle': 2}},\n",
    "             'preprocessing': {3: {0: 'outliers', 1: 'missing_data'}},\n",
    "             'evaluation': {6: ['accuracy',\n",
    "               'f1',\n",
    "               'confusion_matrix',\n",
    "               'classification_report',\n",
    "               'precision',\n",
    "               'recall']},\n",
    "             'eda': {1: {'profiling': 0}},\n",
    "             'modeling': {5: {'hyperparameter_tuning': {}, 'models': {}}},\n",
    "             'storage': {8: {'model': 'FILEPATH',\n",
    "               'pipeline': 'FILEPATH',\n",
    "               'performance': 'FILEPATH',\n",
    "               'data': 'FILEPATH'}},\n",
    "             'feature_engineering': {4: {'transformations': 0,\n",
    "               {'dimensionality_reduction': {1: ['pca', 'svd']}}},\n",
    "             'visualization': {7: ['AUROC']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_exec_hierarchy(hier, config):\n",
    "    exec_plan = {}\n",
    "    \n",
    "    if \n",
    "    for key in hier.keys():\n",
    "        exec_plan\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
